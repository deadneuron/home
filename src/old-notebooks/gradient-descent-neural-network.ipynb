{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent with Neural Networks\n",
    "\n",
    "Given $X$ ant $T$, find $w_k$ that minimizes the squared error in the $k^{th}$ output, then use it to make predictions.\n",
    "\n",
    "Collect all $w_k$ as columns in $W$. $\\tilde{X}$ denotes $X$ with a column of constant 1's prepended as the first column. The target value for the $k^{th}$ output for the $n^{th}$ sample is $t_{n,k}$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  E(W) &= \\sum_{n=1}^N \\sum_{k=1}^K (t_{n,k} - \\tilde{x}_n^T w_k)^2 \\\\\n",
    "   W &= (\\tilde{X}^T \\tilde{X})^{-1} \\tilde{X}^T T \\\\\n",
    "\\\\\n",
    "W &= \\begin{bmatrix}\n",
    "  w_{0,1} & w_{0,2} & \\cdots & w_{0,K}\\\\\n",
    "  w_{1,1} & w_{1,2} & \\cdots & w_{1,K}\\\\\n",
    "  \\vdots\\\\\n",
    "  w_{D,1} & w_{D,2} & \\cdots & w_{D,K}\n",
    "\\end{bmatrix} \\\\\n",
    "\\\\\n",
    "Y &= \\tilde{X} W \\\\\n",
    "\\\\\n",
    "\\tilde{X} & \\text{ is } N \\times (D+1) \\\\\n",
    "W & \\text{ is } (D+1) \\times K \\\\\n",
    "Y & \\text{ is } N \\times K \\\\\n",
    "\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Nonlinear Combinations of Inputs\n",
    "\n",
    "Transform $X$ into $\\Phi(X)$.\n",
    "\n",
    "$$\n",
    "\\Phi(X) = \\Phi\\left ( \n",
    "\\begin{bmatrix}\n",
    "    x_{0,1} & x_{0,2} & \\cdots \\\\\n",
    "    x_{1,1} & x_{1,2} & \\cdots \\\\\n",
    "    \\vdots\\\\\n",
    "    x_{N-1,1} & x_{N-1,2} & \\cdots\n",
    "  \\end{bmatrix}\n",
    "\\right ) = \n",
    "\\begin{bmatrix}\n",
    "    x_{0,1} & x_{0,2}^5  & x_{0,2}^3 x_{0,4}^2 & \\cdots \\\\\n",
    "    x_{1,1} & x_{1,2}^5  & x_{1,2}^3 x_{1,4}^2 & \\cdots\\\\\n",
    "    \\vdots\\\\\n",
    "    x_{N-1,1} & x_{N-1,2}^5 & x_{N-1,2}^3 x_{N-1,4}^2 & \\cdots\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Replace $X$ by $\\Phi$, where $\\Phi_n = \\Phi(x_n)$, and minimize:\n",
    "\n",
    "$$\n",
    "E_k = \\sum_{n=1}^N (t_{n,k} - \\tilde{\\Phi}_n^T w_k)^2\n",
    "$$\n",
    "\n",
    "where we get:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "W &= (\\tilde{\\Phi}^2 \\tilde{\\Phi})^{-1} \\tilde{\\Phi}^T T \\\\\n",
    "Y &= \\tilde{\\Phi} W \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of a Two Layer Network\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\tilde{Z} & = h(\\tilde{X} V)\\\\\n",
    "Y & = \\tilde{Z} W\\\\\n",
    "Y & = \\tilde{h}(\\tilde{X} V) W \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We have a hidden $Z$ and output layer $Y$ with an activation function $h$.\n",
    "\n",
    "$$\n",
    "\\begin{array}{l c c}\n",
    "Name & Function & Derivative \\\\\n",
    "\\hline \\\\\n",
    "\\text{Logistic} & \\sigma(x) = \\frac{1}{1 + e^{-x}} & \\sigma(x)(1 - \\sigma(x)) \\\\\n",
    "\\\\\n",
    "\\text{Hyperbolic Tangent} & tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} & 1 - tanh(x)^2 \\\\\n",
    "\\\\\n",
    "\\text{Rectified Linear Unit} & (x)^+ = max(0, x) & 0 \\text{ if } x < 0 \\text{ else } 1 \\\\\n",
    "\\hline \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "The mean squared error between each target value $t_{n,k}$ and output value $y_{n,k}$ is\n",
    "\n",
    "$$\n",
    "  E = \\frac{1}{N} \\frac{1}{K} \\sum_{n=1}^N \\sum_{k=1}^K ( t_{n,k} - y_{n,k})^2\n",
    "$$\n",
    "\n",
    "Make small changes to weights $v_{j,m}$ and $w_{m,k}$ in the negative gradient direction by step size $\\rho$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  v_{j,m} &\\leftarrow v_{j,m} - \\rho \\frac{\\partial E}{\\partial v_{j,m}}\\\\\n",
    "  w_{m,k} &\\leftarrow w_{m,k} - \\rho \\frac{\\partial E}{\\partial w_{m,k}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Now the derivation\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  E &= \\frac{1}{N} \\frac{1}{K} \\sum_{n=1}^N \\sum_{k=1}^K (t_{n,k} - y_{n,k})^2\\\\\n",
    "  \n",
    "  y_{n,k'} &= \\sum_{m'=0}^M  w_{m',k'} \\tilde{z}_{n,m'}\\\\\n",
    "  \n",
    "  &  = \\sum_{m'=0}^M  w_{m',k'} \\tilde{h}\\left ( \\sum_{j'=0}^D\n",
    "    v_{j,m'} \\tilde{x}_{n,j'} \\right )\\\\\n",
    "\n",
    "  \\frac{\\partial E}{\\partial w_{m,k}} & = -2 \\frac{1}{N} \\frac{1}{K} \\sum_{n=1}^N \\sum_{k'=1}^K (t_{n,k'} - y_{n,k'}) \\frac{\\partial y_{n,k'}}{\\partial w_{m,k}}\\\\\n",
    "  \n",
    "  & = -2 \\frac{1}{N} \\frac{1}{K} \\sum_{n=1}^N \\sum_{k'=1}^K (t_{n,k'} - y_{n,k'}) \n",
    "  \n",
    "  \\frac{\\partial \\left ( \\sum_{m'=0}^M w_{m',k'} \\tilde{z}_{n,m'}  \\right )}{\\partial w_{m,k}}\\\\\n",
    "  \n",
    "  & = -2 \\frac{1}{N} \\frac{1}{K} \\sum_{n=1}^N (t_{n,k} - y_{n,k}) \\tilde{z}_{n,m} \\\\\n",
    "    \\frac{\\partial E}{\\partial v_{j,m}} & = -2 \\frac{1}{N} \\frac{1}{K} \\sum_{n=1}^N \\sum_{k'=1}^K (t_{n,k'} - y_{n,k'}) \\frac{\\partial y_{n,k'}}{\\partial  v_{j,m}}\\\\\n",
    "\n",
    "  & = -2 \\frac{1}{N} \\frac{1}{K} \\sum_{n=1}^N \\sum_{k'=1}^K (t_{n,k'} - y_{n,k'}) \n",
    "  \\frac{\\partial \\left ( \\sum_{m'=0}^M  w_{m',k'} \\tilde{h} \\left ( \\sum_{j'=0}^D v_{j',m'} \\tilde{x}_{n,j'} \\right ) \\right )}{\\partial v_{j,m}}\\\\ \n",
    "  \\text{Let } a_{n,m'} & = \\tilde{h} \\left ( \\sum_{j'=0}^D v_{j',m'} \\tilde{x}_{n,j'}  \\right )\\\\\n",
    "  \\frac{\\partial E}{\\partial v_{j,m}} & = -2 \\frac{1}{N} \\frac{1}{K} \\sum_{n=1}^N \\sum_{k'=1}^K (t_{n,k'} - y_{n,k'})  \\sum_{m'=0}^M  w_{m',k'} \n",
    "  \\frac{\\partial \\tilde{h}(a_{n,m'})}{\\partial  a_{n,m'}} \\frac{\\partial a_{n,m'}}{\\partial v_{j,m}}\\\\\n",
    "  & = -2 \\frac{1}{N} \\frac{1}{K} \\sum_{n=1}^N \\sum_{k'=1}^K (t_{n,k'} - y_{n,k'})  \\sum_{m'=0}^M  w_{m',k'} \\frac{\\partial \\tilde{h}(a_{n,m'})}{\\partial a_{n,m'}} \\tilde{x}_{n,j}\n",
    "\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "      E &= \\frac{1}{N} \\frac{1}{K} \\sum_{n=1}^N \\sum_{k=1}^K (t_{n,k} - y_{n,k})^2\\\\\n",
    "      \\frac{\\partial E}{\\partial w_{m,k}} & = -2 \\frac{1}{N}  \\frac{1}{K}\n",
    "       \\sum_{n=1}^N (t_{n,k} - y_{n,k}) \\tilde{z}_{n,m}\\\\\n",
    "      \\frac{\\partial E}{\\partial v_{j,m}} & = -2 \\frac{1}{N} \\frac{1}{K} \\sum_{n=1}^N \\sum_{k=1}^K (t_{n,k} - y_{n,k})   w_{m,k} (1-z_{n,m}^2) \\tilde{x}_{n,j}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Forward pass:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "      z_{n,m} &= h(\\sum_{j=0}^D v_{j,m} \\tilde{x}_{n,j})\\\\\n",
    "      y_{n,k} &= \\sum_{m=1}^M w_{m,k} \\tilde{z}_{n,m}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Backward pass:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "      v_{j,m} & \\leftarrow v_{j,m} + \\rho_h \\frac{1}{N} \\frac{1}{K} \\sum_{n=1}^N \\sum_{k=1}^K (t_{n,k} - y_{n,k})   w_{m,k} (1-z_{n,m}^2) \\tilde{x}_{n,j}\\\\\n",
    " w_{m,k} & \\leftarrow w_{m,k} + \\rho_o \\frac{1}{N}  \\frac{1}{K}  \\sum_{n=1}^N (t_{n,k} - y_{n,k}) \\tilde{z}_{n,m}\n",
    "\\end{align*}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
