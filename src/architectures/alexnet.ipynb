{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "metadata"
    ]
   },
   "source": [
    "#+NAME: AlexNet\n",
    "#+YEAR: 2012\n",
    "#+AUTHORS: Alex Krishevsky, Ilya Sutskever, Geoffrey Hinton\n",
    "#+CATEGORIES: Convolution\n",
    "#+DESCRIPTION: The model that kickstarted the deep learning revolution. Consisting of eight layers, including five convolutional layers and three fully connected layers. AlexNet was one of the first deep convolutional neural networks to achieve state-of-the-art results on ImageNet. The original implementation split the network over two independent gpus to alleviate challenges with memory at the time.\n",
    "#+PAPER: https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html\n",
    "#+IMAGE: /static/alexnet.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "The model that kickstarted the deep learning revolution. Consisting of eight layers, including five convolutional layers and three fully connected layers. AlexNet was one of the first deep convolutional neural networks to achieve state-of-the-art results on ImageNet. The original implementation split the network over two independent gpus to alleviate challenges with memory at the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use burn::{\n",
    "  config::Config,\n",
    "  module::Module,\n",
    "  nn::{\n",
    "    conv::{Conv2d, Conv2dConfig},\n",
    "    PaddingConfig2d,\n",
    "    pool::{\n",
    "      MaxPool2d, MaxPool2dConfig, \n",
    "      AdaptiveAvgPool2d, AdaptiveAvgPool2dConfig\n",
    "    },\n",
    "    Linear, LinearConfig, \n",
    "    Dropout, DropoutConfig,\n",
    "    ReLU,\n",
    "  },\n",
    "  tensor::{backend::Backend, Tensor},\n",
    "};\n",
    "\n",
    "#[derive(Module, Debug)]\n",
    "pub struct AlexNet<B: Backend> {\n",
    "  conv1: Conv2d<B>,\n",
    "  conv2: Conv2d<B>,\n",
    "  conv3: Conv2d<B>,\n",
    "  conv4: Conv2d<B>,\n",
    "  conv5: Conv2d<B>,\n",
    "  maxpool: MaxPool2d,\n",
    "  avgpool: AdaptiveAvgPool2d,\n",
    "  linear1: Linear<B>,\n",
    "  linear2: Linear<B>,\n",
    "  linear3: Linear<B>,\n",
    "  dropout: Dropout,\n",
    "  activation: ReLU\n",
    "}\n",
    "\n",
    "#[derive(Config, Debug)]\n",
    "pub struct AlexNetConfig {\n",
    "  #[config(default = \"1000\")]\n",
    "  num_classes: usize,\n",
    "  #[config(default = \"0.5\")]\n",
    "  dropout:f64,\n",
    "}\n",
    "\n",
    "impl AlexNetConfig {\n",
    "  pub fn init<B: Backend>(&self, device: &B::Device) -> AlexNet<B> {\n",
    "    AlexNet {\n",
    "      conv1: Conv2dConfig::new([3, 64], [11, 11])\n",
    "        .with_stride([4, 4])\n",
    "        .with_padding(PaddingConfig2d::Same)\n",
    "        .init(device),\n",
    "      conv2: Conv2dConfig::new([64, 192], [5, 5])\n",
    "        .with_padding(PaddingConfig2d::Same)\n",
    "        .init(device),\n",
    "      conv3: Conv2dConfig::new([192, 384], [3, 3])\n",
    "        .with_padding(PaddingConfig2d::Same)\n",
    "        .init(device),\n",
    "      conv4: Conv2dConfig::new([384, 256], [3, 3])\n",
    "        .with_padding(PaddingConfig2d::Same)\n",
    "        .init(device),\n",
    "      conv5: Conv2dConfig::new([256, 256], [3, 3])\n",
    "        .with_padding(PaddingConfig2d::Same)\n",
    "        .init(device),\n",
    "      maxpool: MaxPool2dConfig::new([3, 3])\n",
    "        .with_strides([2, 2])\n",
    "        .init(),\n",
    "      avgpool: AdaptiveAvgPool2dConfig::new([6, 6]).init(),\n",
    "      linear1: LinearConfig::new(256*6*6, 4096).init(device),\n",
    "      linear2: LinearConfig::new(4096, 4096).init(device),\n",
    "      linear3: LinearConfig::new(4096, self.num_classes).init(device),\n",
    "      dropout: DropoutConfig::new(self.dropout).init(),\n",
    "      activation: ReLU::new(),\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "impl<B: Backend> AlexNet<B> {\n",
    "  pub fn forward(&self, x: Tensor<B, 4>) -> Tensor<B, 2> {\n",
    "    let x = self.conv1.forward(x);\n",
    "    let x = self.activation.forward(x);\n",
    "    let x = self.maxpool.forward(x);\n",
    "\n",
    "    let x = self.conv2.forward(x);\n",
    "    let x = self.activation.forward(x);\n",
    "    let x = self.maxpool.forward(x);\n",
    "\n",
    "    let x = self.conv3.forward(x);\n",
    "    let x = self.activation.forward(x);\n",
    "\n",
    "    let x = self.conv4.forward(x);\n",
    "    let x = self.activation.forward(x);\n",
    "\n",
    "    let x = self.conv5.forward(x);\n",
    "    let x = self.activation.forward(x);\n",
    "    let x = self.maxpool.forward(x);\n",
    "\n",
    "    let x = self.avgpool.forward(x);\n",
    "\n",
    "    let [batch_size, channels, height, width] = x.dims();\n",
    "    let x = x.reshape([batch_size, channels * height * width]);\n",
    "    \n",
    "    let x = self.dropout.forward(x);\n",
    "    let x = self.linear1.forward(x);\n",
    "    let x = self.activation.forward(x);\n",
    "\n",
    "    let x = self.dropout.forward(x);\n",
    "    let x = self.linear2.forward(x);\n",
    "    let x = self.activation.forward(x);\n",
    "\n",
    "    self.linear3.forward(x)\n",
    "  }\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rust",
   "language": "rust",
   "name": "rust"
  },
  "language_info": {
   "codemirror_mode": "rust",
   "file_extension": ".rs",
   "mimetype": "text/rust",
   "name": "Rust",
   "pygment_lexer": "rust",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
