{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e9bef94-5c78-4616-824e-be7869b13ece",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "metadata"
    ]
   },
   "source": [
    "#+TITLE: World Models\n",
    "#+CATEGORIES: reinforcement\n",
    "#+DESCRIPTION: The monotonic on-policy reinforcement learning algorithm.\n",
    "#+AUTHORS: David Ha, Jürgen Schmidhuber  \n",
    "#+SOURCE: https://arxiv.org/abs/1803.10122\n",
    "#+DATE: 2024-01-17\n",
    "#+HERO: /static/space-bg.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038a0096-ac99-43d2-be5a-f89a647a9d47",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# World Models\n",
    "\n",
    "Can agents learn inside their own dreams?\n",
    "\n",
    "Vision -> Memory -> Controller\n",
    "\n",
    "Vision = Variational Autoencoder\n",
    "\n",
    "- Compresses each frame it receives at time step $t$ into a low dimensional latent vector $z_t$​​. This compressed representation can be used to reconstruct the original image.\n",
    "\n",
    "Memory = Mixture Density Recurrent Neural Network\n",
    "\n",
    "- Predictive model of the future $z$ vectors that the variational autoencoder is expected to produce.\n",
    "The RNN portion is trained to output a probability density function $p(z)$ instead of a deterministic prediction of $z$.\n",
    "\n",
    "- The probability density function $p(z)$ is approcimated as a mixture of Gaussians, and the RNN is trained to output the probability distribution of the next latent vector $z_{t+1}$​​ given the current and past information made available to it.\n",
    "\n",
    "- The RNN will model $P(z_{t+1}∣a_t,z_t,h_t)$​, where $a_t$​​ is the action taken at time $t$ and $h_t$​​​ is the hidden state of the RNN at time $t$. During sampling, we can adjust a temperature parameter $\\tau$ to control model uncertainty.\n",
    "\n",
    "Controller = Linear Mapping\n",
    "\n",
    "- The controller is deliberately made as simple and small as possible, and trained separately from the vision and memory networks so that most of the agent’s complexity resides in the world model.\n",
    "\n",
    "$$\n",
    "a_t=W_c[z_t h_t] + b_c\n",
    "$$\n",
    "\n",
    "- In this linear model, $W_c$​​ and $b_c$​​ are the weight matrix and bias vector that maps the concatenated input vector $[z_t h_t]$ to the output action vector $a_t$​​​."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
