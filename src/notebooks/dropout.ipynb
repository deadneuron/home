{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8603c23a-ac0c-4be3-bd7d-34b3dc3f0359",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "metadata"
    ]
   },
   "source": [
    "#+TITLE: Dropout\n",
    "#+CATEGORIES: regularization\n",
    "#+DESCRIPTION: Masking random neurons on each forward pass during training.\n",
    "#+AUTHORS: Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov\n",
    "#+SOURCE: https://jmlr.org/papers/v15/srivastava14a.html\n",
    "#+DATE: 2024-01-18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28447764-2dd4-4d0b-8597-a0ec367c8e33",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Dropout\n",
    "\n",
    "Dropout is a classic regularization technique where random neurons are masked for every forward pass through the network. This is an effective and popular method for reducing overfitting as dropping random neurons and their associated connections can help to reduce the co-adaptation of weights. This effectively trains an exponential number of sparse networks which are implicitly ensembled together during inference when dropout is turned off.\n",
    "\n",
    "![Dropout Image](/static/dropout.gif)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
