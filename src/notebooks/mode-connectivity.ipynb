{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42914f74-a8e5-48df-b922-fceeb1eedd71",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "metadata"
    ]
   },
   "source": [
    "#+TITLE: Mode Connectivity\n",
    "#+CATEGORIES: optimization\n",
    "#+TAGS: landscape ensemble\n",
    "#+DESCRIPTION: Local minima in loss landscapes are connected by high accuracy pathways.\n",
    "#+AUTHORS: Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry Vetrov, Andrew Gordon Wilson\n",
    "#+SOURCE: https://arxiv.org/abs/1802.10026\n",
    "#+DATE: 2024-04-06"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdc99db-ad4f-4b59-bd67-d754a85f9e66",
   "metadata": {},
   "source": [
    "# Mode Connectivity\n",
    "\n",
    "The loss landscapes of deep neural networks are highly complex and the geometric properties of them are underexplored. Mode connectivity finds that there are high accuracy pathways that connect the optima found by training independent models. The pathways between these modes can be approximated with simple curves as they are rarely connected linearly.\n",
    "\n",
    "Let $\\hat{w}_1$ and $\\hat{w}_2$ be the weights for two independently trained networks minimized by some loss $\\mathcal{L}(w)$. Let $\\phi_{\\theta}: [0,1] \\rightarrow \\mathbb{R}^{|w|}$ be a continuous piecewise smooth parametric curve with parameters $\\theta$ that correspond to the network weights $\\phi_{\\theta}(0) = \\hat{w}_1$ and $\\phi_{\\theta}(1) = \\hat{w}_2$. The high accuracy pathway is described by the set of curve parameters $\\theta$ that minimize the expectation of the loss with respect to a uniform distribution on the curve.\n",
    "\n",
    "$$\n",
    "l(\\theta) = \\int_0^1 \\mathcal{L}(\\phi_{\\theta}(t))dt\n",
    "$$\n",
    "\n",
    "Many parameterizations are possible with the polygonal chain and the bezier curve being used as an example in the paper. A quadratic Bezier curve provides a natural and smooth parameterization.\n",
    "\n",
    "$$\n",
    "\\phi_{\\theta}(t) = (1-t)^2 \\hat{w}_1 + 2t(1-t)\\theta + t^2 \\hat{w}_2, 0 \\leq t \\leq 1\n",
    "$$\n",
    "\n",
    "The parameters of the curve are optimized with a standard gradient descent based approach where $\\bar{t}$ is sampled from the uniform distribution $U(0, 1)$ at each itearation and steps are taken with respect to the loss $\\mathcal{L}(\\phi_{\\theta}(\\bar{t}))$.\n",
    "\n",
    "These high accuracy pathways can be leveraged to construct low-cost ensembles via a snapshot ensemble style algorithm. Cyclic learning rate schedules are used where large learning rates encourage movement in weight space while the small rates promote convergence to accurate local minima. The learning rate schedule is a shifted annealed cosine where $\\alpha_0$ is the initial learning rate, $t$ is the iteration number, $T$ is the total number of iterations, and $M$ is the number of cycles.\n",
    "\n",
    "$$\n",
    "\\alpha(t) =\\frac{\\alpha_0}{2} \\left( cos \\left( \\frac{\\pi mod(t-1, \\lceil T/M \\rceil )}{\\lceil T/M \\rceil} \\right) + 1 \\right)\n",
    "$$\n",
    "\n",
    "The minima found through this procedure are then used as endpoints for the curve fitting algorithm and all the weights found along the curve can be used to form independent ensemble members. This approach enables much larger ensembles of accurate members without needing to train for additional epochs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
