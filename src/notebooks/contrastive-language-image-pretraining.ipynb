{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13af2794-db7d-4814-b082-b1f64235234f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "metadata"
    ]
   },
   "source": [
    "#+TITLE: Contrastive Language-Image Pretraining\n",
    "#+CATEGORIES: optimization\n",
    "#+TAGS: optimization laguage vision\n",
    "#+DESCRIPTION: Connecting text and images.\n",
    "#+AUTHORS: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\n",
    "#+SOURCE: https://arxiv.org/abs/2103.00020\n",
    "#+DATE: 2024-04-10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7ef1b2-ca8e-401b-8d8a-ace951c676e3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Contrastive Language-Image Pretraining\n",
    "\n",
    "![CLIP](/static/figures/clip.png)\n",
    "\n",
    "This paper presents an effective approach for pretraining vision models by leveraging natural language as a form of supervision. Typical computer vision systems are trained to predict images from a fixed set of predetermined categories. CLIP is instead trained on a large dataset of images that are captioned with natural language taken from the internet. Training involves matching captions with images in a batch which enables models to learn a wide variety of visual concepts in images and associate them with their names. This approach enables zero-shot transfer capabilities where the model can identify and categorize objects it was not explicitly trained to recognize.\n",
    "\n",
    "## Key Ideas\n",
    "\n",
    "- Natural language supervision is massively scalable in this context. A dataset of 400 million (image, text) pairs sourced from the internet enables pretraining on a much larger scale than traditional computer vision datasets with fixed object categories. Learning from raw descriptive text provides a richer and more flexible semantic context which expands beyond standard benchmark datasets. \n",
    "\n",
    "- An image encoder and a text encoder are trained simultaneously to predict correct pairings of images and text within a batch. The model maximizes the similarity between correct (image, text) pairs while minimizing the similarity between mismatched pairs. The objective function is formulated as a symmetric cross entropy loss over the cosine similarity where $I$ and $T$ are the image and text embeddings and $\\tau$ is a tunable temperature parameter.\n",
    "\n",
    "\\begin{gather}\n",
    "\\mathcal{L}_{i,j} = - log \\frac{exp(sim(I_i, T_j) / \\tau)}{\\sum_{i \\neq k} exp(sim(I_i, T_k) / \\tau)} \\\\\n",
    "sim = \\frac{A \\cdot B}{||A|| ||B||}\n",
    "\\end{gather}\n",
    "\n",
    "\n",
    "- The text encoder can be used to generate labels for new images which allows for application to new classification tasks without needing additional specific training.\n",
    "\n",
    "## Results\n",
    "\n",
    "Performance was tested across more than 30 existing computer vision datasets covering tasks from optical character recognition to fine-grained object classification with a variety of model architectures. CLIP matched or exceeded the zero-shot performance of previous models across most evaluated tasks where it demonstrated the capability to generalize from natural language supervision to a wide array of visual tasks without additional task-specific training. Notably, it achieved competitive performance to the original ResNet-50 on ImageNet in a zero-shot setting. CLIP has been a foundational piece of research for modern generative and multimodal learning tasks by learning a rich and semantic understanding of images with textual descriptions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
