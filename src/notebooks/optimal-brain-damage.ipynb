{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a48db88f-b7b8-4059-9072-3c71366fc054",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "metadata"
    ]
   },
   "source": [
    "#+TITLE: Optimal Brain Damage\n",
    "#+CATEGORIES: Compression\n",
    "#+DESCRIPTION: ...\n",
    "#+DATE: 2024-01-29"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc55f4e-56a7-4aba-9b0b-2ee75e233075",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Optimal Brain Damage\n",
    "\n",
    "Optimal Brain Damage is an approach to identify and remove unimportant weights from a neural network. The procedure involves training the network, computing the second derivatives for each parameter, computing the saliencies, sorting the parameters by saliency, and deleting some low-saliency parameters.\n",
    "\n",
    "The saliency of a parameter is defined as the change in the objective function caused by deleting that parameter. Using the second derivative of the objective function with respect to the parameters avoids the prohibitive labor of directly evaluating the saliency by temporarily deleting each parameter and reevaluating the objective function​​.\n",
    "\n",
    "Second order methods are typically difficult with neural networks due to the enormity of the Hessian matrix. Optimal Brain Damage introduces a simple diagonal approximation where the change in the objective function $E$ caused by deleting several parameters is the sum of the changes caused by deleting each parameter individually.\n",
    "\n",
    "The OBD method introduces simplifying approximations due to the practical insolubility of finding a set of parameters whose deletion will cause the least increase of the objective function, E. The difficulty arises from the enormity of the Hessian matrix, H, and its computational challenges. The \"diagonal\" approximation is used where the change in E caused by deleting several parameters is the sum of the changes caused by deleting each parameter individually​​.\n",
    "\n",
    "The method uses a \"quadratic\" approximation, assuming the cost function is nearly quadratic, allowing the neglect of the last term in the equation, simplifying the change in the objective function, ΔEΔE, to ΔE=12∑ihii(Δui)2ΔE=21​∑i​hii​(Δui​)2​​.\n",
    "\n",
    "The second derivatives, crucial for the method, are computed efficiently. The diagonal terms of the Hessian, hiihii​, are given by ∑(i,j)∈Vk∂wij∂uk∂2E∂wij2∑(i,j)∈Vk​​∂uk​∂wij​​∂wij2​∂2E​, where the summand can be expanded as ∂2E∂wij2=zi∂2E∂ai2∂wij2​∂2E​=zi​∂ai2​∂2E​. These second derivatives are back-propagated from layer to layer​\n",
    "\n",
    "The OBD procedure involves training the network, computing the second derivatives for each parameter, computing the saliencies (Sk=huuu2/2Sk​=huu​u2/2), sorting the parameters by saliency, and deleting some low-saliency parameters. This process can be iterated, with several variants possible, such as decreasing the values of low-saliency parameters instead of setting them to zero or allowing deleted parameters to adapt again after being set to zero​\n",
    "​."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
