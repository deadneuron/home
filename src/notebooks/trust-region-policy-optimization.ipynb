{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98f57a9f-f732-4669-9ea7-560626949476",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "metadata"
    ]
   },
   "source": [
    "#+TITLE: Trust Region Policy Optimization\n",
    "#+CATEGORIES: reinforcement\n",
    "#+DESCRIPTION: The monotonic on-policy reinforcement learning algorithm.\n",
    "#+AUTHORS: John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, Pieter Abbeel  \n",
    "#+SOURCE: https://arxiv.org/abs/1502.05477\n",
    "#+DATE: 2024-01-13\n",
    "#+HERO: /static/space-bg.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fb67cb-c766-4867-a653-82e533d422c4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Trust Region Policy Optimization\n",
    "\n",
    "Trust Region Policy Optimization is a key paper in the progression of on-policy algorithms. The promise of monotonic improvement guarantees and better sample efficiency than its predecessors created a powerful and stable training algorithm for reinforcement learning models.\n",
    "\n",
    "### Problems Addressed\n",
    "\n",
    "When using policy gradients, small changes in the parameter space can sometimes have very large differences in performance. This makes it dangerous to use large step sizes as a single bad step can collapse policy performance.\n",
    "\n",
    "### Key Points\n",
    "The trust region policy optimization algorithm aims to update policies by taking the largest step possible while satisfying a constraint on how close the new and old policies are.\n",
    "\n",
    "$$\n",
    "\\underset{\\theta}{maximize} \\ \\mathcal{L}(\\theta, \\theta_{old}) \\ subject \\ to \\ D_{KL}(\\theta \\ || \\ \\theta_{old}) \\leq \\delta\n",
    "$$\n",
    "\n",
    "The surrogate advantage function $\\mathcal{L}$ measures how a policy $\\pi$ performs relative to an old policy $\\pi_{old}$.\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta, \\theta_{old}) = \\mathbb{E}\\left[ \\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_{old}}(a|s)} A_{\\theta_{old}}(s,a) \\right]\n",
    "$$\n",
    "\n",
    "Updates are constrained by the KL divergence, which measures how different a probability distribution $P$ is from another distribution $Q$.\n",
    "\n",
    "$$\n",
    "D_{KL}(P \\ || \\ Q) = \\sum_{x \\in X} P(x) \\ log\\left( \\frac{P(x)}{Q(x)} \\right)\n",
    "$$\n",
    "\n",
    "In this case, we measure the KL divergence between the new and old policy.\n",
    "\n",
    "$$\n",
    "D_{KL}(\\theta \\ || \\ \\theta_{old}) = [D_{KL}(\\pi_{\\theta_{old}}(\\cdot | s) \\ || \\ \\pi_{\\theta}(\\cdot | s))]\n",
    "$$\n",
    "\n",
    "The bound for the KL divergence constraint is an arbitrary constant $\\delta$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
