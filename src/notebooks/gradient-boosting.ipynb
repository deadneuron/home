{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4a96d1e-670d-441a-aa75-4516519afc4b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "metadata"
    ]
   },
   "source": [
    "#+TITLE: Gradient Boosting\n",
    "#+CATEGORIES: optimization ensemble\n",
    "#+DESCRIPTION: ...\n",
    "#+AUTHORS: Leo Breiman\n",
    "#+SOURCE: https://www.stat.berkeley.edu/~breiman/arcing-the-edge.pdf\n",
    "#+DATE: 2024-02-19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063266a2-6686-448e-b20b-a4e755eec799",
   "metadata": {},
   "source": [
    "# Gradient Boosting\n",
    "\n",
    "Gradient Boosting is an approach in ensemble learning where models are trained in succession with adaptively weighted training sets to correct the errors of previous models. At its core, gradient boosting applies the principle of gradient descent to the functional space of predictive models.\n",
    "\n",
    "Boosting algorithms typically use large populations of weak learners like shallow decision trees and many variations of boosting has been introduced with different approach to reweighting training data. AdaBoost is perhaps the most popular approach. Consider a dataset $\\{(x_1, y_1), ..., (x_n, y_n)\\}$ where a set of weak classifiers $\\{k_1, k_n\\}$ are combined to output a classification for each item.\n",
    "\n",
    "$$\n",
    "C_{m-1}(x_i) = \\alpha_1 k_1(x_i) + ... + \\alpha_n k_n(x_i)\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
