{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "metadata"
    ]
   },
   "source": [
    "#+TITLE: Natural Evolution Strategies\n",
    "#+CATEGORIES: optimization neuroevolution\n",
    "#+TAGS: evolution\n",
    "#+DESCRIPTION: A family of algorithms for evolving the parameters of search distributions.\n",
    "#+AUTHORS: Daan Wierstra, Tom Schaul, Tobias Glasmachers, Yi Sun, JÃ¼rgen Schmidhuber\n",
    "#+SOURCE: https://arxiv.org/abs/1106.4487\n",
    "#+DATE: 2024-01-19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Evolution Strategies\n",
    "\n",
    "![NES](/static/figures/nes.png)\n",
    "\n",
    "A family of numerical optimization algorithms for black-box problems which iteratively update a search distribution by using an estimated gradient on its distribution parameters.\n",
    "\n",
    "- A parameterized search distribution is used to produce a batch of search points.\n",
    "- The fitness function is evaluated at each search point.\n",
    "- A search gradient is estimated using the parameters of the search points with respect to the expected fitness.\n",
    "- A gradient ascent step is taken along the natural gradient, which is a second order method that renormalizes the update with respect to uncertainty (prevents oscillations, premature convergence, and undesirable local effects).\n",
    "\n",
    "The core idea is to use search gradients to update parameters where the search gradient is the sampled gradient of expected fitness. Let $\\theta$ be the parameters of the search distribution $\\pi(x|\\theta)$ and $f(x)$ the fitness function evaluated at $x$. The objective is to maximize the expected fitness under the distribution.\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\mathbb{E}_{\\theta}[f(x)] = \\int f(x) \\ \\pi(x | \\theta) \\ dx\n",
    "$$\n",
    "\n",
    "The gradient can be written as:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\theta}[f(z) \\ \\nabla_{\\theta} \\ log \\ \\pi(z|\\theta)]\n",
    "$$\n",
    "\n",
    "The Monte Carlo estimate from samples $z_1, ..., z_{\\lambda}$ as:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta} J(\\theta) \\approx \\frac{1}{\\lambda} \\sum_{k=1}^{\\lambda} f(z_k) \\ \\nabla_{\\theta} \\ log \\ \\pi(z_k | \\theta)\n",
    "$$\n",
    "\n",
    "The parameters are then updated with a standard ascent scheme:\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta + \\eta \\nabla_{\\theta} J(\\theta)\n",
    "$$\n",
    "\n",
    "The natural gradient accounts for uncertainty in the second order updates by removing the dependence on parameterization by instead relying on the KL divergence between probability distributions where the Fisher information matrix defines the local curvature in distribution space.\n",
    "\n",
    "\\begin{align}\n",
    "F &= \\int \\pi(z|\\theta) \\ \\nabla_{\\theta} \\ log \\ \\pi(z|\\theta) \\ \\nabla_{\\theta} \\ log \\ \\pi(z|\\theta)^{\\top} dz \\\\\n",
    "&\\approx \\frac{1}{\\lambda} \\sum_{k=1}^{\\lambda} \\nabla_{\\theta} \\ log \\ \\pi(z_k|\\theta) \\ \\nabla_{\\theta} \\ log \\ \\pi(z_k|\\theta)^{\\top}\n",
    "\\end{align}\n",
    "\n",
    "The ascent scheme then becomes:\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta + \\eta F^{-1} \\nabla_{\\theta} J(\\theta)\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
