{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "metadata"
    ]
   },
   "source": [
    "#+NAME: Perceptron\n",
    "#+YEAR: 1958\n",
    "#+AUTHORS: Frank Rosenblatt\n",
    "#+CATEGORIES: Fully Connected\n",
    "#+DESCRIPTION: Neural network research begins with the first implementation of an artificial neuron called the perceptron. The theory for the perceptron was introduced in 1943 by McCulloch and Pitts as a binary threshold classifier. The first implementation was actually intended to be a machine rather than a program. Photocells were interconnected with potentiometers that were updated during learning with electric motors.\n",
    "#+PAPER: /static/perceptron.pdf\n",
    "#+IMAGE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron\n",
    "\n",
    "The origins of artificial neural network research can be traced back to the work of Warren McCulloch and Walter Pitts in 1943, who proposed a mathematical model of a biological neuron based on the all-or-nothing character of nervous activity. This simplified computational model casts the neuron as a binary threshold unit that computes a weighted sum of its inputs to produce an output.\n",
    "\n",
    "$$\n",
    "f(x) = \\begin{cases}\n",
    "1,  &\\text{if} \\ \\displaystyle\\sum_{i=0}^n w_i x_i + b > 1, \\\\\n",
    "0, &\\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transition to multilayer perceptrons greatly expanded the learning capabilities of neural networks. Networks would now be organized into several successive layers, each consisting of multiple neurons with weighted connections to neurons in adjacent layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "ArgumentError: Package CUDA not found in current path.\n- Run `import Pkg; Pkg.add(\"CUDA\")` to install the CUDA package.",
     "output_type": "error",
     "traceback": [
      "ArgumentError: Package CUDA not found in current path.\n- Run `import Pkg; Pkg.add(\"CUDA\")` to install the CUDA package.",
      "",
      "Stacktrace:",
      " [1] macro expansion",
      "   @ Base ./loading.jl:1766 [inlined]",
      " [2] macro expansion",
      "   @ Base ./lock.jl:267 [inlined]",
      " [3] __require(into::Module, mod::Symbol)",
      "   @ Base ./loading.jl:1747",
      " [4] #invoke_in_world#3",
      "   @ Base ./essentials.jl:921 [inlined]",
      " [5] invoke_in_world",
      "   @ Base ./essentials.jl:918 [inlined]",
      " [6] require(into::Module, mod::Symbol)",
      "   @ Base ./loading.jl:1740"
     ]
    }
   ],
   "source": [
    "# This will prompt if neccessary to install everything, including CUDA:\n",
    "using Flux, CUDA, Statistics, ProgressMeter\n",
    "\n",
    "# Generate some data for the XOR problem: vectors of length 2, as columns of a matrix:\n",
    "noisy = rand(Float32, 2, 1000)                                    # 2×1000 Matrix{Float32}\n",
    "truth = [xor(col[1]>0.5, col[2]>0.5) for col in eachcol(noisy)]   # 1000-element Vector{Bool}\n",
    "\n",
    "# Define our model, a multi-layer perceptron with one hidden layer of size 3:\n",
    "model = Chain(\n",
    "    Dense(2 => 3, tanh),   # activation function inside layer\n",
    "    BatchNorm(3),\n",
    "    Dense(3 => 2),\n",
    "    softmax) |> gpu        # move model to GPU, if available\n",
    "\n",
    "# The model encapsulates parameters, randomly initialised. Its initial output is:\n",
    "out1 = model(noisy |> gpu) |> cpu                                 # 2×1000 Matrix{Float32}\n",
    "\n",
    "# To train the model, we use batches of 64 samples, and one-hot encoding:\n",
    "target = Flux.onehotbatch(truth, [true, false])                   # 2×1000 OneHotMatrix\n",
    "loader = Flux.DataLoader((noisy, target) |> gpu, batchsize=64, shuffle=true);\n",
    "# 16-element DataLoader with first element: (2×64 Matrix{Float32}, 2×64 OneHotMatrix)\n",
    "\n",
    "optim = Flux.setup(Flux.Adam(0.01), model)  # will store optimiser momentum, etc.\n",
    "\n",
    "# Training loop, using the whole data set 1000 times:\n",
    "losses = []\n",
    "@showprogress for epoch in 1:1_000\n",
    "    for (x, y) in loader\n",
    "        loss, grads = Flux.withgradient(model) do m\n",
    "            # Evaluate model and loss inside gradient context:\n",
    "            y_hat = m(x)\n",
    "            Flux.crossentropy(y_hat, y)\n",
    "        end\n",
    "        Flux.update!(optim, model, grads[1])\n",
    "        push!(losses, loss)  # logging, outside gradient context\n",
    "    end\n",
    "end\n",
    "\n",
    "optim # parameters, momenta and output have all changed\n",
    "out2 = model(noisy |> gpu) |> cpu  # first row is prob. of true, second row p(false)\n",
    "\n",
    "mean((out2[1,:] .> 0.5) .== truth)  # accuracy 94% so far!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.0",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
