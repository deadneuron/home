[{"name": "DenseNet", "year": "2016", "authors": "Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Weinberger", "categories": "Convolutional", "description": "DenseNet builds on top of the ideas introduced in residual networks by implementing skip connections between a given layer and every other successive layer following it. Each layer receives a concatenation of all the feature maps of all the layers preceding it. This allows for learning at different levels of abstraction throughout the network.", "paper": "/static/densenet.pdf", "image": "/static/densenet.png", "slug": "/models/densenet"}, {"name": "SqueezeNet", "year": "2016", "authors": "Forrest Iandola, Song Han, Matthew Moskewicz, Khalid Ashraf, William Dally, Kurt Keutzer", "categories": "Convolutional", "description": "SqueezeNet is notable for achieving comparable accuracy to larger neural network architectures while using significantly fewer parameters. SqueezeNet utilizes fire modules that consist of a 1x1 convolutional squeeze layer followed by a 3x3 convolutional expand layer.", "paper": "/static/squeezenet.pdf", "image": null, "slug": "/models/squeezenet"}, {"name": "U-Net", "year": "2015", "authors": "Olaf Ronneberger, Philipp Fischer, Thomas Brox", "categories": "Convolutional", "description": "U-Net is widely used in various segmentation and image-to-image translation tasks. The architecture consists of a contracting path that uses convolutional and pooling layers to capture context and a symmetric expanding path that uses deconvolutional and upsampling layers to achieve localization. The architecture's unique U-shape allows it to preserve fine-grained details during upsampling and is particularly well-suited for segmenting structures with varying shapes and sizes.", "paper": "/static/unet.pdf", "image": "/static/unet.png", "slug": "/models/unet"}, {"name": "ResNet", "year": "2015", "authors": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun", "categories": "Convolutional", "description": "Residual Networks popularized skip connections in deep networks to alleviate vanishing gradient problems. The shortcut connections between blocks allows the gradients to flow directly from later layers to earlier layers, enabling depths of over a hundred layers.", "paper": "/static/resnet.pdf", "image": "/static/resnet.png", "slug": "/models/resnet"}, {"name": "Inception", "year": "2014", "authors": "Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich", "categories": "Convolutional", "description": "Inception is a deep convolutional network developed by Google researchers. It introduced the concept of inception modules, which are composed of multiple parallel convolutional layers of different kernel sizes to capture different feature scales.", "paper": "/static/inception.pdf", "image": "/static/inception.png", "slug": "/models/inception"}, {"name": "AlexNet", "year": "2012", "authors": "Alex Krishevsky, Ilya Sutskever, Geoffrey Hinton", "categories": "Convolutional", "description": "The model that kickstarted the deep learning revolution. Consisting of eight layers, including five convolutional layers and three fully connected layers. AlexNet was one of the first deep convolutional neural networks to achieve state-of-the-art results on ImageNet. The original implementation split the network over two independent gpus to alleviate challenges with memory at the time.", "paper": "/static/alexnet.pdf", "image": "/static/alexnet.png", "slug": "/models/alexnet"}, {"name": "Perceptron", "year": "1958", "authors": "Frank Rosenblatt", "categories": "Linear", "description": "Neural network research begins with the first implementation of an artificial neuron called the perceptron. The theory for the perceptron was introduced in 1943 by McCulloch and Pitts as a binary threshold classifier. The first implementation was actually intended to be a machine rather than a program. Photocells were interconnected with potentiometers that were updated during learning with electric motors.", "paper": "/static/perceptron.pdf", "image": null, "slug": "/models/perceptron"}]