<!DOCTYPE html>
<html>
  <head>
    <title>Models | Dead Neuron</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link rel="icon" href="/static/favicon.ico" />
    <link rel="stylesheet" href="/static/style.css" />
    <link rel="stylesheet" href="/static/models.css" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" />
    <link
      rel="stylesheet"
      href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@500;700&amp;display=block"
    />
  </head>
  <body>
    <div class="home">
      <header>
        <div class="wrapper">
          <a href="/" class="logo"><img src="/static/logo.svg" /></a>
          <nav>
            <a href="/about">About</a>
            <a href="/models">Models</a>
            <a href="/notebooks">Notebooks</a>
            <a href="/research">Research</a>
          </nav>
        </div>
      </header>
      <main>
        <div class="hero">
          <div class="overlay"></div>
          <div class="wrapper">
            <h1>Models</h1>
            <p>
              Highlighting some of the most interesting and impactful neural
              network architectures throughout the history of artificial
              intelligence research.
            </p>
          </div>
        </div>
        <div id="react-feed">
          <div class="catalog wrapper">
            <div class="filters">
              <nav class="categories">
                <span class="active">All</span><span>Attention</span
                ><span>Convolutional</span><span>Energy</span
                ><span>Fully Connected</span><span>Recurrent</span>
              </nav>
            </div>
            
            <div class="item">
              <div class="content">
                <h1>CoAtNet (2021)</h1>
                <p><small>Zihang Dai, Hanxiao Liu, Quoc V. Le, Mingxing Tan</small></p>
                <p>CoAtNets feature the unification of depthwise convolutions and self attention via relative attention in this hybrid network structure. Convolution layers and attention layers are vertically stacked in a principled way that proves to be effective for improving generalization, capacity, and efficiency.</p>
                <div class="links">
                  <!-- <a href="/models/coatnet">Read More</a> -->
                  <a href="/static/coatnet.pdf">Paper</a>
                </div>
              </div>
              
            </div>
            
            <div class="item">
              <div class="content">
                <h1>MLP Mixer (2021)</h1>
                <p><small>Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, Alexey Dosovitskiy</small></p>
                <p>A new take on vision architectures that eschew convolution and attention for only fully connected layers. MLP mixer applies spatial embeddings to the input image and then applies permutation-invariant operations through channel and token mixing layers.</p>
                <div class="links">
                  <!-- <a href="/models/mlpmixer">Read More</a> -->
                  <a href="/static/mlpmixer.pdf">Paper</a>
                </div>
              </div>
              
              <div class="image">
                <img src="/static/mlp-mixer.png" />
              </div>
              
            </div>
            
            <div class="item">
              <div class="content">
                <h1>Capsule Network (2017)</h1>
                <p><small>Sara Sabour, Nicholas Frosst, Geoffrey E Hinton</small></p>
                <p>Capsule networks were introduced to solve some of the limitations that convolutional networks have in understanding hierarchies between objects in images. These capsules use small groups of neurons that specialize in identifying various parts of an object and their spatial relationships.</p>
                <div class="links">
                  <!-- <a href="/models/capsule">Read More</a> -->
                  <a href="/static/capsule.pdf">Paper</a>
                </div>
              </div>
              
            </div>
            
            <div class="item">
              <div class="content">
                <h1>ShuffleNet (2017)</h1>
                <p><small>Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, Jian Sun</small></p>
                <p>ShuffleNet is a mobile optimized network that uses a combination of depthwise separable convolutions, channel shuffle operations, and pointwise convolutions to greatly reduce computational cost while maintaining accuracy.</p>
                <div class="links">
                  <!-- <a href="/models/squeezenet">Read More</a> -->
                  <a href="/static/shufflenet.pdf">Paper</a>
                </div>
              </div>
              
            </div>
            
            <div class="item">
              <div class="content">
                <h1>Transformer (2017)</h1>
                <p><small>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Lion Jones, Aidan Gomez, Lukasz Kaiser, Illia Polosukhin</small></p>
                <p>These models replace the traditional recurrent and convolutional networks with a self attention mechanism that allows the model to attend to different parts of the input sequence to generate contextual representations. The transformer consists of an encoder and decoder architecture that perform input/output mapping.</p>
                <div class="links">
                  <!-- <a href="/models/transformer">Read More</a> -->
                  <a href="/static/transformer.pdf">Paper</a>
                </div>
              </div>
              
            </div>
            
            <div class="item">
              <div class="content">
                <h1>DenseNet (2016)</h1>
                <p><small>Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Weinberger</small></p>
                <p>DenseNet builds on top of the ideas introduced in residual networks by implementing skip connections between a given layer and every other successive layer following it. Each layer receives a concatenation of all the feature maps of all the layers preceding it. This allows for learning at different levels of abstraction throughout the network.</p>
                <div class="links">
                  <!-- <a href="/models/densenet">Read More</a> -->
                  <a href="/static/densenet.pdf">Paper</a>
                </div>
              </div>
              
              <div class="image">
                <img src="/static/densenet.png" />
              </div>
              
            </div>
            
            <div class="item">
              <div class="content">
                <h1>SqueezeNet (2016)</h1>
                <p><small>Forrest Iandola, Song Han, Matthew Moskewicz, Khalid Ashraf, William Dally, Kurt Keutzer</small></p>
                <p>SqueezeNet is notable for achieving comparable accuracy to larger neural network architectures while using significantly fewer parameters. SqueezeNet utilizes fire modules that consist of a 1x1 convolutional squeeze layer followed by a 3x3 convolutional expand layer.</p>
                <div class="links">
                  <!-- <a href="/models/shufflenet">Read More</a> -->
                  <a href="/static/squeezenet.pdf">Paper</a>
                </div>
              </div>
              
            </div>
            
            <div class="item">
              <div class="content">
                <h1>U-Net (2015)</h1>
                <p><small>Olaf Ronneberger, Philipp Fischer, Thomas Brox</small></p>
                <p>U-Net is widely used in various segmentation and image-to-image translation tasks. The architecture consists of a contracting path that uses convolutional and pooling layers to capture context and a symmetric expanding path that uses deconvolutional and upsampling layers to achieve localization. The architecture's unique U-shape allows it to preserve fine-grained details during upsampling and is particularly well-suited for segmenting structures with varying shapes and sizes.</p>
                <div class="links">
                  <!-- <a href="/models/unet">Read More</a> -->
                  <a href="/static/unet.pdf">Paper</a>
                </div>
              </div>
              
              <div class="image">
                <img src="/static/unet.png" />
              </div>
              
            </div>
            
            <div class="item">
              <div class="content">
                <h1>ResNet (2015)</h1>
                <p><small>Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun</small></p>
                <p>Residual Networks popularized skip connections in deep networks to alleviate vanishing gradient problems. The shortcut connections between blocks allows the gradients to flow directly from later layers to earlier layers, enabling depths of over a hundred layers.</p>
                <div class="links">
                  <!-- <a href="/models/resnet">Read More</a> -->
                  <a href="/static/resnet.pdf">Paper</a>
                </div>
              </div>
              
              <div class="image">
                <img src="/static/resnet.png" />
              </div>
              
            </div>
            
            <div class="item">
              <div class="content">
                <h1>Inception (2014)</h1>
                <p><small>Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich</small></p>
                <p>Inception is a deep convolutional network developed by Google researchers. It introduced the concept of inception modules, which are composed of multiple parallel convolutional layers of different kernel sizes to capture different feature scales.</p>
                <div class="links">
                  <!-- <a href="/models/inception">Read More</a> -->
                  <a href="/static/inception.pdf">Paper</a>
                </div>
              </div>
              
              <div class="image">
                <img src="/static/inception.png" />
              </div>
              
            </div>
            
            <div class="item">
              <div class="content">
                <h1>Gated Recurrent Unit (2014)</h1>
                <p><small>Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio</small></p>
                <p>The GRU is a streamlined variant of the LSTM recurrent network. It contains memory cells that combine the input and forget gates into a single update gate which merges hidden states. This gate reduction results in fewer parameters, faster training times and reduced computational complexity.</p>
                <div class="links">
                  <!-- <a href="/models/gru">Read More</a> -->
                  <a href="/static/gru.pdf">Paper</a>
                </div>
              </div>
              
              <div class="image">
                <img src="/static/gru.png" />
              </div>
              
            </div>
            
            <div class="item">
              <div class="content">
                <h1>VGG (2014)</h1>
                <p><small>Karen Simonyan, Andrew Zisserman</small></p>
                <p>The Visual Geometry Group developed this network architecture that is characterized by its simplicity and uniformity. This model popularized the 3x3 convolutional filter size and its design has influenced the development of modern convolutional architectures.</p>
                <div class="links">
                  <!-- <a href="/models/vgg">Read More</a> -->
                  <a href="/static/vgg.pdf">Paper</a>
                </div>
              </div>
              
              <div class="image">
                <img src="/static/vgg.png" />
              </div>
              
            </div>
            
            <div class="item">
              <div class="content">
                <h1>AlexNet (2012)</h1>
                <p><small>Alex Krishevsky, Ilya Sutskever, Geoffrey Hinton</small></p>
                <p>The model that kickstarted the deep learning revolution. Consisting of eight layers, including five convolutional layers and three fully connected layers. AlexNet was one of the first deep convolutional neural networks to achieve state-of-the-art results on ImageNet. The original implementation split the network over two independent gpus to alleviate challenges with memory at the time.</p>
                <div class="links">
                  <!-- <a href="/models/alexnet">Read More</a> -->
                  <a href="/static/alexnet.pdf">Paper</a>
                </div>
              </div>
              
              <div class="image">
                <img src="/static/alexnet.png" />
              </div>
              
            </div>
            
            <div class="item">
              <div class="content">
                <h1>Echo State Network (2002)</h1>
                <p><small>Herbert Jaeger</small></p>
                <p>The echo state network is a type of recurrent architecture that utilizes a reservoir of fixed and randomly weighted connections that remain frozen during training. Only layers outside of the reservoir are optimized. The dynamic behavior of the reservoir generates complex temporal patterns which create an echo of the input signal.</p>
                <div class="links">
                  <!-- <a href="/models/echostate">Read More</a> -->
                  <a href="/static/echostate.pdf">Paper</a>
                </div>
              </div>
              
              <div class="image">
                <img src="/static/echostate.png" />
              </div>
              
            </div>
            
            <div class="item">
              <div class="content">
                <h1>Long Short-Term Memory (1991)</h1>
                <p><small>Sepp Hochreiter, Jürgen Schmidhuber</small></p>
                <p>The LSTM is a recurrent network that contains a memory cell and three gating units (input, output, and forget) that regulate the flow of information into and out of the cell. The memory cell can selectively remember or forget information over long periods of time.</p>
                <div class="links">
                  <!-- <a href="/models/lstm">Read More</a> -->
                  <a href="/static/lstm.pdf">Paper</a>
                </div>
              </div>
              
              <div class="image">
                <img src="/static/lstm.png" />
              </div>
              
            </div>
            
            <div class="item">
              <div class="content">
                <h1>Boltzmann Machine (1985)</h1>
                <p><small>David Ackley, Geoffrey Hinton, Terrence Sejnowski</small></p>
                <p>The Boltzmann Machine is a type of probabilistic graph model that consists of binary stochastic units and weights learned through an energy based minimization algorithm called contrastive divergence. The name comes from using the Boltzmann distribution to model the joint probability distribution of the input data.</p>
                <div class="links">
                  <!-- <a href="/models/boltzmann">Read More</a> -->
                  <a href="/static/boltzmann.pdf">Paper</a>
                </div>
              </div>
              
            </div>
            
            <div class="item">
              <div class="content">
                <h1>LeNet (1985)</h1>
                <p><small>Yann LeCun, Bernhard Boser, John Denker, Donnie Henderson, Richard Howard, Wayne Hubbard, Lawrence Jackel</small></p>
                <p>LeNet is the earliest introduction of the modern convolutional architecture. It contains three alternating convolutional and pooling layers followed by two fully connected layers. This model was introduced alongside the back propagation algorithm which produced incredible results at the time for image classification.</p>
                <div class="links">
                  <!-- <a href="/models/lenet">Read More</a> -->
                  <a href="/static/boltzmann.pdf">Paper</a>
                </div>
              </div>
              
            </div>
            
            <div class="item">
              <div class="content">
                <h1>Hopfield Network (1982)</h1>
                <p><small>John Hopfield</small></p>
                <p>The Hopfield network is a type of recurrent network based on bidirectional connections between neurons. Hopfield networks are a type of Ising model (also known as spin glass) that use Hebbian learning to train associative memory systems.</p>
                <div class="links">
                  <!-- <a href="/models/hopfield">Read More</a> -->
                  <a href="/static/hopfield.pdf">Paper</a>
                </div>
              </div>
              
            </div>
            
            <div class="item">
              <div class="content">
                <h1>Neocognitron (1979)</h1>
                <p><small>Kunihiko Fukushima</small></p>
                <p>A lesser known piece of history. The earliest inspiration for convolutional neural networks comes from Japan in the midst of the AI winter. A breakthrough development for computer vision, the neocognitron utilizes alternating layers of locally connected feature extraction and positional shift cells.</p>
                <div class="links">
                  <!-- <a href="/models/neocognitron">Read More</a> -->
                  <a href="/static/neocognitron.pdf">Paper</a>
                </div>
              </div>
              
            </div>
            
            <div class="item">
              <div class="content">
                <h1>Perceptron (1958)</h1>
                <p><small>Frank Rosenblatt</small></p>
                <p>Neural network research begins with the first implementation of an artificial neuron called the perceptron. The theory for the perceptron was introduced in 1943 by McCulloch and Pitts as a binary threshold classifier. The first implementation was actually intended to be a machine rather than a program. Photocells were interconnected with potentiometers that were updated during learning with electric motors.</p>
                <div class="links">
                  <!-- <a href="/models/perceptron">Read More</a> -->
                  <a href="/static/perceptron.pdf">Paper</a>
                </div>
              </div>
              
            </div>
            
          </div>
        </div>
      </main>
      <script src="/static/prism.js"></script>
      <script src="/static/prism-python.min.js"></script>
      <script src="/static/prism-julia.min.js"></script>
      <footer>
        <div class="wrapper">
          <div class="column">
            <h2>Site</h2>
            <a href="/about">About</a><a href="/models">Models</a
            ><a href="/notebooks">Notebooks</a><a href="/research">Research</a>
          </div>
          <div class="column">
            <h2>Links</h2>
            <a href="https://arxiv-sanity-lite.com">Arxiv Sanity</a
            ><a href="https://paperswithcode.com">Papers With Code</a
            ><a href="https://midjourney.com">Midjourney</a
            ><a href="https://openai.com">OpenAI</a>
          </div>
          <div class="column">
            <h2>Self</h2>
            <a href="/cv">CV</a
            ><a href="https://github.com/tjwhitaker">Github</a
            ><a href="https://lichess.org/@/tjwhitaker">Lichess</a
            ><a href="https://orcid.org/0000-0003-3792-3901">Orcid</a>
          </div>
        </div>
      </footer>
      <script src="https://unpkg.com/react@16/umd/react.development.js"></script>
      <script src="https://unpkg.com/react-dom@16/umd/react-dom.development.js"></script>
      <script src="/static/models-feed.js"></script>
    </div>
  </body>
</html>