<!DOCTYPE html><html><head><title>Home | Dead Neuron</title><meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" /><link rel="icon" href="/static/favicon.ico" /><link rel="stylesheet" href="/static/blog.css" /><link rel="preconnect" href="https://fonts.googleapis.com" /><link rel="preconnect" href="https://fonts.gstatic.com" /><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@500;700&amp;display=block" /></head><body><div class="wrapper"><header><a href="/" class="logo">Dead Neuron</a></header><main><h1 class="article-title">Neural Style Transfer</h1><p><em>Neural Style Transfer is an algorithm to transfer the style from one image onto the content of another. It was introduced a 2015 paper called A Neural Algorithm of Artistic Style by Leon A. Gatys, Alexander S. Ecker, Matthias Bethge.</em></p>
<p><img src="/static/style-transfer.png" /></p>
<p>Neural Style Transfer was one of the first projects I came across that got me interested in machine learning. I used to be pretty into photoshop and photo editing, so I was really drawn to this simple yet elegant idea of transferring image styles. I had a lot of fun playing with and learning about neural style transfer and I think it's a great example of how neural networks can be used to produce creative results.</p>
<p>So how does it work?</p>
<p>The crux of the work here lies in how to define the style and the content of an image. One solution is to use the intermediate layer outputs of a pretrained computer vision model. These intermediate layers contain feature maps which get progressively more higher-level as you move through the network. Thus we will use the early layers in the network to define a style representation and we will use the last layer to represent the content.</p>
<p>We'll define two loss terms that utilize the pre-trained model's intermediate layer outputs and we'll use those loss terms to try and create an input image that minimizes those losses.</p>
<p>The content loss is quite simple. It's gotten by passing the input image and the content image through the network and taking the euclidean distance between the outputs at the content layer for each.</p>
<p>The style loss is a bit more involved. It's gotten by passing the input image and the style image through the networks and comparing the gram matrices of the outputs of all the style layers. The gram matrix essentially measures the correlation between different filter map responses and if those filter maps fire in a similar way, then we will have an image with more of that style.</p>
<p>We then optimize these losses like we would with any other neural network problem, but instead of optimizing the weights of the neural network, we instead optimize the pixels of the input image.</p>
<p>For implementation details, check out <a href="https://github.com/leongatys/PytorchNeuralStyleTransfer" class="uri">https://github.com/leongatys/PytorchNeuralStyleTransfer</a>.</p>
</main><aside><div class="meta"><h3 class="minion">Meta</h3><p>Hello and welcome! My name is Tim and I'm trying to showcase some cool and innovative applications of machine learning in art and science.</p></div><div class="feed"><h3 class="minion">Feed</h3><a class="post" href="/neural-style-transfer"><h3>Neural Style Transfer</h3><p>A Neural Algorithm of Artistic Style</p><small>2021-03-01</small></a><a class="post" href="/the-higgs-boson-challenge"><h3>The Higgs Boson Challenge</h3><p>Hello world</p><small>2021-03-01</small></a><a class="post" href="/open-exoplanet-catalogue"><h3>Open Exoplanet Catalogue</h3><p>Hello world</p><small>2021-03-01</small></a><a class="post" href="/neural-darwinism"><h3>Neural Darwinism</h3><p>An neuro-evolutionary theory published in 1978 by nobel prize winning biologist Gerald Edelman.</p><small>2021-01-10</small></a></div></aside></div></body></html>