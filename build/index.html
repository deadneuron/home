<!DOCTYPE html><html><head><title>Wits End</title><meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" /><link rel="icon" href="/static/favicon.ico" /><link rel="stylesheet" href="/static/style.css" /><link rel="stylesheet" href="/static/home.css" /><link rel="preconnect" href="https://fonts.googleapis.com" /><link rel="preconnect" href="https://fonts.gstatic.com" /><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@500;700&amp;display=block" /><link rel="stylesheet" href="https://api.mapbox.com/mapbox-gl-js/v2.1.1/mapbox-gl.css" /><script src="https://api.mapbox.com/mapbox-gl-js/v2.1.1/mapbox-gl.js"></script></head><body><div class="home"><header><div class="wrapper"><a href="/" class="logo"><img src="/static/logo.svg" /></a><nav><a href="/cv">CV</a><a href="/research">Publications</a><a href="/models">Models</a><a href="/notebooks">Notebooks</a></nav></div></header><main><div class="hero"><div class="overlay"></div><div class="wrapper"><h1>Building Better<br />Neural Networks</h1><p>Welcome! My name is Tim and Wits End is my personal website, a repository for my research, and an outlet for digging deeper into artificial intelligence.</p></div></div><div class="content wrapper"><div class="header"><h1>Articles</h1></div><div class="primary-col"><div class="paper interpretable-diversity-analysis"><div><h1>Knowledge Distillation: Teaching Small Models to Perform Like Large Ones</h1><p>IJCNN 2022</p><p>Knowledge Distillation is a model compression technique in machine learning where a smaller, simpler student model is trained to replicate the behavior of a larger, more complex teacher model. The fundamental principle underlying this technique is the transference of 'knowledge' from the teacher model to the student model...</p><a href="#" class="read-more">Read More</a></div></div><div class="paper synaptic-stripping"><div><h1>Sparsity in Practice: Leveraging Sparse Representations for Efficient Neural Networks</h1><p>IJCNN 2023</p><p>A deep dive into the concept of sparsity in neural networks, providing practical advice on designing and training sparse models for improved efficiency. The resultant networks not only reduce the memory footprint and computational requirements but often also improve generalization by reducing overfitting...</p><a href="#" class="read-more">Read More</a></div></div></div><div class="secondary-col"><div class="paper sparse-mutation-decomposition"><div><h1>A Deep Dive into Activation Functions</h1><a href="#" class="read-more">Read More</a></div></div><div class="paper synaptic-stripping"><div><h1>Binary and Ternary Networks: The Future of Ultra-Efficient Machine Learning</h1><a href="#" class="read-more">Read More</a></div></div><div class="paper low-cost-ensembles"><div><h1>Batch Normalization: A Key to Faster and More Stable Training</h1><a href="#" class="read-more">Read More</a></div></div><div class="paper stochastic-masking"><div><h1>Adaptive Learning Rates: The Power of Optimizers in Neural Networks</h1><a href="#" class="read-more">Read More</a></div></div><div class="paper quantum-neuron-selection"><div><h1>Exploring the Benefits of Quantization in Neural Networks</h1><a href="#" class="read-more">Read More</a></div></div></div></div><div class="featured-paper prune-and-tune-ensembles"><div class="wrapper"><div><h1>Subnetwork Ensembles</h1><p>Ensemble Learning is an effective method for improving generalization in machine learning. However, as state-of-the-art neural networks grow larger, the computational cost associated with training several independent networks becomes expensive. We introduce and formalize a fast, low-cost method for creating diverse ensembles of neural networks without needing to train multiple models from scratch...</p><a href="#" class="read-more">Read More</a></div></div></div><div class="intro"><div class="wrapper"><div class="column-wrapper"><div class="column"><div class="icon"><img src="/static/crypto-gpu.svg" /></div><h2>Model Optimization</h2><p>I'm passionate about optimizing learning algorithms and model architectures for superior performance.</p></div><div class="column"><div class="icon"><img src="/static/statistical-analysis.svg" /></div><h2>Data Analysis</h2><p>I love diving into complex datasets and uncovering meaningful insights that drive successful machine learning projects.</p></div><div class="column"><div class="icon"><img src="/static/documents-ui.svg" /></div><h2>Software Engineering</h2><p>I have over a decade of experience in designing, developing, and deploying robust and maintainable software.</p></div></div></div></div><div class="reference"><p>Hero images generated with neural networks via <a href="https://midjourney.com">midjourney</a>. This website's <a href="https://github.com/tjwhitaker/thewhiteacre">source code</a> was written entirely in scheme!</p></div></main><footer><div class="wrapper"><div class="column"><h2>Site</h2><a href="/about">About</a><a href="/models">Models</a><a href="/notebooks">Notebooks</a><a href="/research">Research</a></div><div class="column"><h2>Links</h2><a href="https://arxiv-sanity-lite.com">Arxiv Sanity</a><a href="https://paperswithcode.com">Papers With Code</a><a href="https://news.ycombinator.com">Hacker News</a><a href="https://www.youtube.com/c/pbsspacetime">Space Time</a></div><div class="column"><h2>Self</h2><a href="#">CV</a><a href="https://github.com/tjwhitaker">Github</a><a href="https://lichess.org/@/tjwhitaker">Lichess</a><a href="https://orcid.org/0000-0003-3792-3901">Orcid</a></div></div></footer><script>	mapboxgl.accessToken = 'pk.eyJ1IjoidG13aHRrciIsImEiOiJja2x2NzdpaW0wNXRnMndwOGszNTc3aWd5In0.LvJ2znCQ_1v9a86fxUhQ2A';
            var map = new mapboxgl.Map({
            container: 'map', // container id
            style: 'mapbox://styles/mapbox/streets-v12', // style URL
            center: [-121.4944, 38.5816], // starting position [lng, lat]
            zoom: 10 // starting zoom
            });</script></div></body></html>