<!DOCTYPE html><html><head><title>Wits End</title><meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" /><link rel="icon" href="/static/favicon.ico" /><link rel="stylesheet" href="/static/style.css" /><link rel="stylesheet" href="/static/home.css" /><link rel="preconnect" href="https://fonts.googleapis.com" /><link rel="preconnect" href="https://fonts.gstatic.com" /><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@500;700&amp;display=block" /><link rel="stylesheet" href="https://api.mapbox.com/mapbox-gl-js/v2.1.1/mapbox-gl.css" /><script src="https://api.mapbox.com/mapbox-gl-js/v2.1.1/mapbox-gl.js"></script></head><body><div class="home"><header><div class="wrapper"><a href="/" class="logo"><img src="/static/logo.svg" /></a><nav><a href="/cv">CV</a><a href="/research">Publications</a><a href="/models">Models</a><a href="/notebooks">Notebooks</a></nav></div></header><main><div class="hero"><div class="overlay"></div><div class="wrapper"><h1>Building Better<br />Neural Networks</h1><p>Welcome! My name is Tim and Wits End is my personal website, a repository for my research, and an outlet for digging deeper into artificial intelligence.</p></div></div><div class="intro"><div class="wrapper"><div class="column-wrapper"><div class="column"><div class="icon"><img src="/static/crypto-gpu.svg" /></div><h2>Model Optimization</h2><p>I'm passionate about optimizing learning algorithms and model architectures for superior performance.</p></div><div class="column"><div class="icon"><img src="/static/statistical-analysis.svg" /></div><h2>Data Analysis</h2><p>I love diving into complex datasets and uncovering meaningful insights that drive successful machine learning projects.</p></div><div class="column"><div class="icon"><img src="/static/documents-ui.svg" /></div><h2>Software Engineering</h2><p>I have over a decade of experience in designing, developing, and deploying robust and maintainable software.</p></div></div></div></div><div class="featured-paper prune-and-tune-ensembles"><div class="wrapper"><div><h1>Machine Learning Research For Good</h1><p>Ensemble Learning is an effective method for improving generalization in machine learning. However, as state-of-the-art neural networks grow larger, the computational cost associated with training several independent networks becomes expensive. We introduce and formalize a fast, low-cost method for creating diverse ensembles of neural networks without needing to train multiple models from scratch...</p><a href="#" class="read-more">Read More</a></div></div></div><div class="notebooks wrapper"><div class="header"><nav class="categories"><a href="#" class="active">All</a><a href="#">Architecture</a><a href="#">Compression</a><a href="#">Evolution</a><a href="#">Optimization</a><a href="#">Regularization</a></nav></div><div class="primary-col"><div class="post"><small>2020-01-02</small><h1>Data Augmentation</h1><p>A strategy used in machine learning to increase the diversity and amount of training data without actually collecting new data. It involves creating modified versions of existing data using techniques like rotation, scaling, flipping, cropping, and brightness or color adjustments.</p><a href="/notebooks/data-augmentation">Read More</a></div><div class="post"><small>2020-01-02</small><h1>Quantization</h1><p>Quantization is used to reduce the precision of the weights and biases in a model in order to decrease computational requirements. It involves converting full-precision 32-bit weights into lower-precision formats. Typically 16-bit or 8-bit quantization is used, but research has shown promise in resource constrained enviroments for ternary and binary networks.</p><a href="/notebooks/quantization">Read More</a></div></div><div class="secondary-col"><div class="post"><small>2020-01-02</small><h1>Knowledge Distillation</h1><a href="/notebooks/knowledge-distillation">Read More</a></div><div class="post"><small>2020-01-02</small><h1>Weight Pruning</h1><a href="/notebooks/weight-pruning">Read More</a></div><div class="post"><small>2020-01-02</small><h1>Convolutions</h1><a href="/notebooks/convolutions">Read More</a></div><div class="post"><small>2020-01-01</small><h1>Activation Functions</h1><a href="/notebooks/activation-functions">Read More</a></div></div></div><div class="reference"><p>Hero images generated with neural networks via <a href="https://midjourney.com">midjourney</a>.</p></div></main><footer><div class="wrapper"><div class="column"><h2>Site</h2><a href="/about">About</a><a href="/models">Models</a><a href="/notebooks">Notebooks</a><a href="/research">Research</a></div><div class="column"><h2>Links</h2><a href="https://arxiv-sanity-lite.com">Arxiv Sanity</a><a href="https://paperswithcode.com">Papers With Code</a><a href="https://news.ycombinator.com">Hacker News</a><a href="https://www.youtube.com/c/pbsspacetime">Space Time</a></div><div class="column"><h2>Self</h2><a href="#">CV</a><a href="https://github.com/tjwhitaker">Github</a><a href="https://lichess.org/@/tjwhitaker">Lichess</a><a href="https://orcid.org/0000-0003-3792-3901">Orcid</a></div></div></footer></div></body></html>