<!DOCTYPE html>
<html>
  <head>
    <title>Dead Neuron</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link rel="icon" href="/static/favicon.ico" />
    <link rel="stylesheet" href="/static/style.css" />
    <link rel="stylesheet" href="/static/home.css" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" />
    <link
      rel="stylesheet"
      href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@500;700&amp;display=block"
    />
  </head>
  <body>
    <div class="home">
      <header>
        <div class="wrapper">
          <div class="container">
            <a href="/" class="logo"><img src="/static/logo.svg" /></a>
            <nav>
              <a href="/about">About</a>
              <a href="/models">Models</a>
              <a href="/notebooks">Notebooks</a>
              <a href="/research">Research</a>
            </nav>
          </div>
        </div>
      </header>      <main>
        <div class="hero">
          <div class="overlay"></div>
          <div class="wrapper">
            <h1>
              A Stochastic Gradient <br />
              Descent Into Madness
            </h1>
            <p>
              Welcome! My name is Tim Whitaker and this is my personal website where I'm sharing insights, tips, and tricks for making the
              most of your machine learning models. Read on to explore articles
              and research on neural network optimization.
            </p>
          </div>
        </div>
        <div id="react-feed">
          <div class="feed wrapper">
            <div class="filters">
              <nav class="categories">
                <span class="active">All</span><span>Architecture</span
                ><span>Compression</span><span>Optimization</span
                ><span>Regularization</span>
              </nav>
              <nav class="mobile-categories"><h1>Recent Posts</h1></nav>
            </div>
            <div class="primary-col">
              
              <div class="post">
                <small class="date">2024-01-14</small>
                <small class="categories">optimization</small>
                <h1 class="title">Stochastic Weight Averaging</h1>
                <p class="description">An optimization trick for the final phases of training with SGD.</p>
                <a href="/notebooks/stochastic-weight-averaging">Read More</a>
              </div>
              
              <div class="post">
                <small class="date">2020-01-02</small>
                <small class="categories">compression</small>
                <h1 class="title">Quantization</h1>
                <p class="description">Quantization is used to reduce the precision of the weights and biases in a model in order to decrease computational requirements. It involves converting full-precision 32-bit weights into lower-precision formats. Typically 16-bit or 8-bit quantization is used, but research has shown promise in resource constrained enviroments for ternary and binary networks.</p>
                <a href="/notebooks/quantization">Read More</a>
              </div>
              
            </div>
            <div class="secondary-col">
              
              <div class="post">
                <small class="date">2024-01-13</small>
                <small class="categories">reinforcement</small>
                <h1 class="title">Trust Region Policy Optimization</h1>
                <p class="description">The monotonic on-policy reinforcement learning algorithm.</p>
                <a href="/notebooks/trust-region-policy-optimization">Read More</a>
              </div>
              
              <div class="post">
                <small class="date">2023-12-13</small>
                <small class="categories">architecture</small>
                <h1 class="title">Activation Functions</h1>
                <p class="description">Activation functions play a crucial role in determining the output of neurons in neural networks. They introduce non-linearity into the network, enabling it to model complex relationships between inputs and outputs.</p>
                <a href="/notebooks/activation-functions">Read More</a>
              </div>
              
              <div class="post">
                <small class="date">2020-01-02</small>
                <small class="categories">optimization</small>
                <h1 class="title">Learning Rate Schedules</h1>
                <p class="description">These strategies adjust the learning rate, and/or other optimizer parameters, throughout the training process. Generally, large learning rates are used in the early stages of training where large steps can be taken to improve efficiency before slowly decaying to small rates to encourage fine-grained convergence.</p>
                <a href="/notebooks/learning-rate-schedules">Read More</a>
              </div>
              
              <div class="post">
                <small class="date">2020-01-02</small>
                <small class="categories">architecture</small>
                <h1 class="title">Convolutions</h1>
                <p class="description">Convolutions are mathematical operations that process input data using filters or kernels. The operation involves sliding the filter over the input data and performing an element-wise multiplication between the filter weights and the input window. These filters often learn to detect edges, shapes, or textures in image data in a translation invariant way.</p>
                <a href="/notebooks/convolutions">Read More</a>
              </div>
              
            </div>
          </div>
        </div>
        <div class="featured-paper subnetwork-ensembles">
          <div class="wrapper">
            <div>
              <h2>PhD Dissertation</h2>
              <h1>Subnetwork Ensembles</h1>
              <p>
                Neural network ensembles have been effectively used to improve
                generalization by combining the predictions of multiple
                independently trained models. However, the growing scale and
                complexity of deep neural networks have led to these methods
                becoming prohibitively expensive and time consuming to
                implement. Low-cost ensemble methods have become increasingly
                important as they can alleviate the need to train multiple
                models from scratch while retaining the generalization benefits
                that...
              </p>
              <a
                href="https://mountainscholar.org/items/4a5ca7f3-0c31-4389-be7f-06405baa87a7"
                class="read-more"
                >Read More</a
              >
            </div>
          </div>
        </div>
      </main>
      <footer>
        <div class="wrapper">
          <div class="column">
            <h2>Site</h2>
            <a href="/about">About</a>
            <a href="/models">Models</a>
            <a href="/notebooks">Notebooks</a>
            <a href="/research">Research</a>
          </div>
          <div class="column">
            <h2>Links</h2>
            <a href="https://arxiv-sanity-lite.com">Arxiv Sanity</a>
            <a href="https://paperswithcode.com">Papers With Code</a>
            <a href="https://midjourney.com">Midjourney</a>
            <a href="https://openai.com">OpenAI</a>
          </div>
          <div class="column">
            <h2>Self</h2>
            <a href="/cv">CV</a>
            <a href="https://github.com/tjwhitaker">Github</a>
            <a href="https://lichess.org/@/tjwhitaker">Lichess</a>
            <a href="https://orcid.org/0000-0003-3792-3901">Orcid</a>
          </div>
        </div>
      </footer>
      <script src="https://unpkg.com/react@16/umd/react.development.js"></script>
      <script src="https://unpkg.com/react-dom@16/umd/react-dom.development.js"></script>
      <script src="/static/home-feed.js"></script>
    </div>
  </body>
</html>