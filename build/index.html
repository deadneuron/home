<!DOCTYPE html><html><head><title>Wittes Ende</title><meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" /><link rel="icon" href="/static/favicon.ico" /><link rel="stylesheet" href="/static/home.css" /><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Fira+Mono:400,500,700|&amp;display=swap" /><link rel="stylesheet" href="https://api.mapbox.com/mapbox-gl-js/v2.1.1/mapbox-gl.css" /><script src="https://api.mapbox.com/mapbox-gl-js/v2.1.1/mapbox-gl.js"></script></head><body><div class="home"><header><div class="wrapper"><a href="/" class="logo"><img src="/static/logo.svg" /></a><nav><a href="/about">About</a><a href="/art">Art</a><a href="/music">Music</a><a href="/writing">Writing</a></nav></div></header><main><div class="hero"><div class="overlay"></div><div class="wrapper"><h1>Machine Learning For Science</h1><p>Welcome to Wittes Ende! My name is Tim and I'm a computer scientist studying neural networks and machine learning. Read on to check out a collection of my research papers.</p></div></div><div class="prune-and-tune-ensembles"><div class="wrapper"><div><h1>Prune and Tune Ensembles</h1><p>Ensemble Learning is an effective method for improving generalization in machine learning. However, as state-of-the-art neural networks grow larger, the computational cost associated with training several independent networks becomes expensive. We introduce a fast, low-cost method for creating diverse ensembles of neural networks without needing to train multiple models from scratch. We do this by first training a single parent network. We then create child networks by cloning the parent and...</p><a href="/static/prune-and-tune-ensembles.pdf" class="read-more">Read More</a></div></div></div><div class="interpretable-diversity-analysis"><div class="wrapper"><div><h1>Interpretable Diversity Analysis</h1><p>Diversity is an important consideration in the construction of robust neural network ensembles. A collection of well trained models will generalize better to unseen data if they are diverse in the patterns they respond to and the predictions they make. Encouraging diversity becomes especially important for low-cost ensemble methods, as members often share network structure or training epochs in order to avoid training several independent networks from scratch...</p><a href="/static/there-is-no-magic-subnetwork.pdf" class="read-more">Read More</a></div></div></div><div class="quantum-neuron-selection"><div class="wrapper"><div><h1>Quantum Neuron Selection</h1><p>Gradient descent methods have long been the de facto standard for training deep neural networks. Millions of training samples are fed into models with billions of parameters, which are slowly updated over hundreds of epochs. Recently, it's been shown that large, randomly initialized, neural networks contain subnetworks that perform as well as fully trained models. This insight offers a promising avenue for...</p><a href="/static/quantum-neuron-selection.pdf" class="read-more">Read More</a></div></div></div></main><footer><div class="wrapper"><div class="column"><h2>Site</h2><a href="/about">About</a><a href="/art">Art</a><a href="/music">Music</a><a href="">Contact</a></div><div class="column"><h2>Links</h2><a href="">Hello</a><a href="">Hello</a></div><div class="column"><h2>Hello World</h2></div></div></footer></div></body></html>