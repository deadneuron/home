<!DOCTYPE html><html><head><title>Notebooks | Wits End</title><meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" /><link rel="icon" href="/static/favicon.ico" /><link rel="stylesheet" href="/static/style.css" /><link rel="stylesheet" href="/static/notebooks.css" /><link rel="preconnect" href="https://fonts.googleapis.com" /><link rel="preconnect" href="https://fonts.gstatic.com" /><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@500;700&amp;display=block" /><link rel="stylesheet" href="https://api.mapbox.com/mapbox-gl-js/v2.1.1/mapbox-gl.css" /><script src="https://api.mapbox.com/mapbox-gl-js/v2.1.1/mapbox-gl.js"></script></head><body><div class="home"><header><div class="wrapper"><a href="/" class="logo"><img src="/static/logo.svg" /></a><nav><a href="/cv">CV</a><a href="/research">Publications</a><a href="/models">Models</a><a href="/notebooks">Notebooks</a></nav></div></header><main><div class="hero"><div class="overlay"></div><div class="wrapper"><h1>Notebooks</h1><p>Code, math, and machine learning tutorials.</p></div></div><div class="feed"><div class="wrapper"><div class="post"><small>2020-01-02</small><h1>Data Augmentation</h1><p>A strategy used in machine learning to increase the diversity and amount of training data without actually collecting new data. It involves creating modified versions of existing data using techniques like rotation, scaling, flipping, cropping, and brightness or color adjustments.</p><a href="/notebooks/data-augmentation">Read More</a></div><div class="post"><small>2020-01-02</small><h1>Quantization</h1><p>Quantization is used to reduce the precision of the weights and biases in a model in order to decrease computational requirements. It involves converting full-precision 32-bit weights into lower-precision formats. Typically 16-bit or 8-bit quantization is used, but research has shown promise in resource constrained enviroments for ternary and binary networks.</p><a href="/notebooks/quantization">Read More</a></div><div class="post"><small>2020-01-02</small><h1>Knowledge Distillation</h1><p>This is a technique used to train a small student model to reproduce the behavior of a larger, more complex teacher model. The student model learns not just from the ground-truth labels but also from the output probabilities generated by the teacher. This allors the smaller model to capture more granular information about the data that it might have otherwise been unable to learn. The primary goal of knowledge distillation is to create lightweight models that are computationally efficient while retaining the high performance of the larger models.</p><a href="/notebooks/knowledge-distillation">Read More</a></div><div class="post"><small>2020-01-02</small><h1>Weight Pruning</h1><p>Pruning is used to reduce the complexity and size of a model by removing weights or neurons. Pruning methods typically select weights to prune according to importance heuristics like magnitude or gradient saliency. However, even random pruning has been shown to produce accurate models at significant levels of sparsity. While pruning can cause some loss in model accuracy, this can be mitigated by fine-tuning the pruned model on the original dataset.</p><a href="/notebooks/weight-pruning">Read More</a></div><div class="post"><small>2020-01-02</small><h1>Convolutions</h1><p>Convolutions are mathematical operations that process input data using filters or kernels. The operation involves sliding the filter over the input data and performing an element-wise multiplication between the filter weights and the input window. This is followed by a sum over the channels to produce a condensed feature map. These filters often learn to detect edges, shapes, or textures in image data and a convolutional layer contains multiple filters to produce complex feature represntations. Convolutional layers significantly reduce the number of parameters compared to fully connected layers and they are translation invariant which can significantly reduce overfitting and improve generalization.</p><a href="/notebooks/convolutions">Read More</a></div><div class="post"><small>2020-01-01</small><h1>Activation Functions</h1><p>Activation functions play a crucial role in determining the output of neurons in neural networks. They introduce non-linearity into the network, enabling it to model complex relationships between inputs and outputs.</p><a href="/notebooks/activation-functions">Read More</a></div></div></div></main><div class="reference"><p>Hero images generated with neural networks via <a href="https://midjourney.com">midjourney</a>. This website's <a href="https://github.com/tjwhitaker/thewhiteacre">source code</a> was written entirely in scheme!</p></div><footer><div class="wrapper"><div class="column"><h2>Site</h2><a href="/about">About</a><a href="/models">Models</a><a href="/notebooks">Notebooks</a><a href="/research">Research</a></div><div class="column"><h2>Links</h2><a href="https://arxiv-sanity-lite.com">Arxiv Sanity</a><a href="https://paperswithcode.com">Papers With Code</a><a href="https://news.ycombinator.com">Hacker News</a><a href="https://www.youtube.com/c/pbsspacetime">Space Time</a></div><div class="column"><h2>Self</h2><a href="#">CV</a><a href="https://github.com/tjwhitaker">Github</a><a href="https://lichess.org/@/tjwhitaker">Lichess</a><a href="https://orcid.org/0000-0003-3792-3901">Orcid</a></div></div></footer></div></body></html>