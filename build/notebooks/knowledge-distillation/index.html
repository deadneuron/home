<!DOCTYPE html><html><head><title>Knowledge Distillation | Wits End</title><meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" /><link rel="icon" href="/static/favicon.ico" /><link rel="stylesheet" href="/static/style.css" /><link rel="stylesheet" href="/static/notebook.css" /><link rel="stylesheet" href="/static/prism.css" /><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css" /><link rel="preconnect" href="https://fonts.googleapis.com" /><link rel="preconnect" href="https://fonts.gstatic.com" /><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@500;700&amp;display=block" /><script src="/static/iframeresizer.min.js"></script></head><body><div class="home"><header><div class="wrapper"><a href="/" class="logo"><img src="/static/logo.svg" /></a><nav><a href="/cv">CV</a><a href="/research">Publications</a><a href="/models">Models</a><a href="/notebooks">Notebooks</a></nav></div></header><main><div class="hero" style="background-image: url(/static/space-bg.png)"><div class="overlay"></div><div class="wrapper"><h1>Knowledge Distillation</h1><p>This is a technique used to train a small student model to reproduce the behavior of a larger, more complex teacher model. The student model learns not just from the ground-truth labels but also from the output probabilities generated by the teacher. This allors the smaller model to capture more granular information about the data that it might have otherwise been unable to learn. The primary goal of knowledge distillation is to create lightweight models that are computationally efficient while retaining the high performance of the larger models.</p></div></div><div class="article"><div class="wrapper"><div class="content"><iframe id="myIframe" src="/notebooks/knowledge-distillation/content.html"></iframe><script>iFrameResize({log: true}, '#myIframe')</script></div></div></div></main><div class="reference"><p>Hero images generated with neural networks via <a href="https://midjourney.com">midjourney</a>. This website's <a href="https://github.com/tjwhitaker/thewhiteacre">source code</a> was written entirely in scheme!</p></div><footer><div class="wrapper"><div class="column"><h2>Site</h2><a href="/about">About</a><a href="/models">Models</a><a href="/notebooks">Notebooks</a><a href="/research">Research</a></div><div class="column"><h2>Links</h2><a href="https://arxiv-sanity-lite.com">Arxiv Sanity</a><a href="https://paperswithcode.com">Papers With Code</a><a href="https://news.ycombinator.com">Hacker News</a><a href="https://www.youtube.com/c/pbsspacetime">Space Time</a></div><div class="column"><h2>Self</h2><a href="#">CV</a><a href="https://github.com/tjwhitaker">Github</a><a href="https://lichess.org/@/tjwhitaker">Lichess</a><a href="https://orcid.org/0000-0003-3792-3901">Orcid</a></div></div></footer></div></body></html>